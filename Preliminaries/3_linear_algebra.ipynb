{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalars are the numbers of the everyday mathematics. Scalars are denote by ordinary lower-cased letters (e.g., $x$,$y$ and $z$) and the space of all (continuous) real-valued scalars by $\\Bbb{R}$. The scalars are defined in this space in this way $x \\in \\Bbb{R}$.\n",
    "\n",
    "Scalars are implemented as tensors that contain only one element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x+y, x*y, x/y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For current purposes, you can think of a vector as a fixed-length array of scalars. As with their code counterparts, we call these scalars the elements of the vector (synonyms include entries and components). When vectors represent examples from real-world datasets, their values hold some real-world significance.\n",
    "\n",
    "Vectors are implemented as 1st-order tensors. In general, such tensors can have arbitrary lengths, subject to memory limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We refer to a vector with a bold lowercase letter as $\\mathbf{x}$\n",
    "- We  refer to an element of a vector using a subscript as $x_2$\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix}x_{0} \\\\ \\vdots \\\\ x_{n-1}\\end{bmatrix},$$\n",
    "\n",
    "- Here $x_0, \\ldots, x_{n-1}$ are elements of the vector. Later on, we will distinguish between such column vectors and row vectors whose elements are stacked horizontally. Recall that we access a tensor’s elements via indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(2), tensor(2), tensor(1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], x[2], x[-1], x[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To indicate that a vector contains elements, we write $\\mathbf{x} \\in \\Bbb{R}^{n}$ Formally, we call $n$ the dimensionality of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x) , x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use order to refer to the number of axes and dimensionality exclusively to refer to the number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We denote matrices by bold capital letters as $\\mathbf{A}$\n",
    "- The expresion $\\mathbf{A} \\in \\Bbb{R}^{m \\times n}$ indicates a matrix $\\mathbf{A}$ contains $m \\times n$ real-valued scalars, arranged as $m$ rows and $n$ columns.\n",
    "- When $m = n$ we say that the matrix is square.\n",
    "- We can ilustrate any matrix as a table.\n",
    "- To refer to an individual element we subscript bot the row and column indices $a_{ij}$ element in $\\mathbf{A}$'s $i^{th}$ row and $j^{th}$ column\n",
    "\n",
    "$$\\begin{split}\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.\\end{split}$$\n",
    "\n",
    "- In code, we represent a matrix $\\mathbf{A} \\in \\Bbb{R}^{m \\times n}$ by a 2nd order tensor whit shape $(m,n)$\n",
    "- Matrices are useful for representing datasets. Typically, rows correspond to individual records and columns correspond to distinct attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3,2)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reshape(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reshape(1,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reshape(6,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose\n",
    "\n",
    "- Sometimes we want to flip the axes. When we exchange a matrix’s rows and columns, the result is called its transpose.\n",
    "- Formally, we signify a matrix $\\mathbf{A}$’s transpose by $\\mathbf{A}^{T}$ and if $\\mathbf{B} = \\mathbf{A}^{T}$, then $b_{ij} = a_{ij}$ for all $i$ and $j$. \n",
    "- The transpose of an $m \\times n$ matrix is an $n \\times m$ matrix:\n",
    "\n",
    "$$\\begin{split}\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\\end{split}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symmetric matrices**\n",
    "\n",
    "Symmetric matrices are the subset of square matrices that are equal to their own transposes: $\\mathbf{A} = \\mathbf{A}^\\top$\n",
    ". The following matrix is symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A == A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensors give us a generic way of describing extensions to n-order arrays\n",
    "- We call software objects of the tensor class “tensors” precisely because they too can have arbitrary numbers of axes.\n",
    "- It may be confusing to use the word tensor for both the mathematical object and its realization in code, our meaning should usually be clear from context.\n",
    "- We denote general tensor as $\\mathsf{X}$, $\\mathsf{Y}$ and $\\mathsf{Z}$\n",
    "- Their indexing mechanism follow nasturally from that of matrices $x_{ijk}$\n",
    "\n",
    "- An example of an application of tensors are images.\n",
    "- Each image arrives as a 3rd-order tensor with axes corresponding to the height, width, and channel. At each spatial location, the intensities of each color (red, green, and blue) are stacked along the channel.\n",
    "- A collection of images is represented in code by a \n",
    "4th-order tensor, where distinct images are indexed along the first axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5]],\n",
       "\n",
       "        [[ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(4, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11],\n",
       "         [12, 13],\n",
       "         [14, 15]],\n",
       "\n",
       "        [[16, 17],\n",
       "         [18, 19],\n",
       "         [20, 21],\n",
       "         [22, 23]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(3, 4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tensor Arithmetic Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise operations\n",
    "- Elementwise operations produce outputs that have the same shape as their operands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B, A-B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadarmard product\n",
    "\n",
    "- The Hadamard product (denoted $\\odot$) is the elementwise product of two matrices.\n",
    "- The Hadamard product of two matrices $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ is defined as:\n",
    "\n",
    "$$\\begin{split}\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\\end{split}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  4.],\n",
       "         [ 9., 16., 25.]]),\n",
       " tensor([[ 0.,  1.,  4.],\n",
       "         [ 9., 16., 25.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B, B*A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar addition and multiplication\n",
    "\n",
    "- Adding or multiplying a scalar and a tensor produces a result with the same shape as the original tensor. Here, each element of the tensor is added to (or multiplied by) the scalar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " tensor([[[ 0,  2,  4,  6],\n",
       "          [ 8, 10, 12, 14],\n",
       "          [16, 18, 20, 22]],\n",
       " \n",
       "         [[24, 26, 28, 30],\n",
       "          [32, 34, 36, 38],\n",
       "          [40, 42, 44, 46]]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, a * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a+X).shape, (a*X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sum of tensor elements, considering a vecor $\\mathbf{x}$ of lenght $n$, we write $\\sum_{i=0}^{n-1} x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To express sums over the elements of tensors of arbitrary shape, we simply sum over all its axes. \n",
    "- Considering $\\mathbf{A}$ as $m \\times n$ matrix, the sum is written: $\\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} a_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " torch.Size([2, 3]),\n",
       " tensor(15.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2,3)\n",
    "A, A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Invoking the sum function reduces a tensor along all of its axes, eventually producing a scalar\n",
    "- To sum over all elements along the rows (axis 0), we specify axis=0 in sum. Since the input matrix reduces along axis 0 to generate the output vector, this axis is missing from the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " torch.Size([2, 3]),\n",
       " tensor([3., 5., 7.]),\n",
       " torch.Size([3]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.shape, A.sum(axis=0), A.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " torch.Size([2, 3]),\n",
       " tensor([ 3., 12.]),\n",
       " torch.Size([2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.shape, A.sum(axis=1), A.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reducing a matrix along both rows and columns via summation is equivalent to summing up all the elements of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) == A.sum()  # Same as A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A related quantity is the mean, also called the average. We calculate the mean by dividing the sum by the total number of elements.\n",
    "- Likewise, the function for calculating the mean can also reduce a tensor along specific axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.5000), tensor(2.5000))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 4.]), tensor([1., 4.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=1), A.sum(axis=1) / A.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Reduction Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean. This matters when we want to use the broadcast mechanism.\n",
    "- Mantains the number of axes but not but not the exact dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3., 12.]),\n",
       " tensor([[ 3.],\n",
       "         [12.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=1), A.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, (A.sum(axis=1)).shape, (A.sum(axis=1, keepdims=True)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.],\n",
       "        [12.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For instance, since sum_A keeps its two axes after summing each row, we can divide A by sum_A with broadcasting to create a matrix where each row sums up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.3333, 0.6667],\n",
       "         [0.2500, 0.3333, 0.4167]]),\n",
       " tensor([1., 1.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A, (A / sum_A).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we want to calculate the cumulative sum of elements of A along some axis, say axis=0 (row by row), we can call the cumsum function. By design, this function does not reduce the input tensor along any axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[0., 1., 2.],\n",
       "         [3., 5., 7.]]),\n",
       " tensor([[ 0.,  1.,  3.],\n",
       "         [ 3.,  7., 12.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.cumsum(axis=0), A.cumsum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the most fundamental operations is the dot product.\n",
    "- Given two vectos $\\mathbf{x}$,$\\mathbf{y} \\in \\Bbb{R}^d$ their dot product (or inner product) $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$ is a sum over the products of the elements at the same position:\n",
    "$$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=0}^{d-1} x_i y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype = torch.float32)\n",
    "y = torch.ones(3, dtype = torch.float32) \n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Equivalently we can calculate the dot product of two vectors performing an elementwise multiplication followed by sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts:\n",
    "- Given some set of values, denoted by a vector $\\mathbf{x} \\in \\Bbb{R}^n$, and a set of weights, denoted by $\\mathbf{w} \\in \\Bbb{R}^n$, the weighted sum of the values in $\\mathbf{x}$ according to the weights $\\mathbf{w}$ could be expressed as the dot product $\\mathbf{x}^{\\top}\\mathbf{w}$. When the weights are nonnegative and sum to 1, the dot product expresses a weighted average.\n",
    "- After normalizing two vectors to have unit length, the dot product express the cosine of the angle between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Vector Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To start off, we visualize our matrix in terms of its row vectors.\n",
    "\n",
    "$$\\begin{split}\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},\\end{split}$$\n",
    "\n",
    "- Each $\\mathbf{a}^\\top_{i} \\in \\Bbb{R}^{n}$ is a row vector representing the $i^{th}$ row of the matrix $\\mathbf{A}$ \n",
    "- The vector $\\mathbf{x}$ is an $n$-dimensional vector\n",
    "- The matrix-vector porduct $\\mathbf{A}\\mathbf{x}$ is a column vector of lenght $m$, whose $i^{th}$ element is the dot product $\\mathbf{a^{\\top}_i}\\mathbf{x}$\n",
    "\n",
    "$$\\begin{split}\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\\end{split}$$\n",
    "\n",
    "\n",
    "- We can think this multiplication as transformation that project vectors from $\\Bbb{R}^{n}$ to $\\Bbb{R}^{m}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([0., 1., 2.]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype = torch.float32).reshape(2,3)\n",
    "x = torch.arange(3, dtype = torch.float32)\n",
    "A, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(14.), tensor(5.), tensor(14.))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A[0]*x).sum(), (A[1]*x).sum(), torch.dot(A[0],x), torch.dot(A[1],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 14.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(A,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 14.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The matrix produce a transformation in the vector\n",
    "- We can represent rotations as multiplications by certain square matrices.\n",
    "- Matrix–vector products also describe the key calculation involved in computing the outputs of each layer in a neural network given the outputs from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Say that we have two matrices $\\mathbf{A} \\in \\Bbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\Bbb{R}^{k \\times m}$\n",
    "\n",
    "\\begin{split}\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.\\end{split}\n",
    "\n",
    "- Let $\\mathbf{a_i^{\\top} \\in \\Bbb{R}^k}$ denote the row vector representing the $i^{th}$ row of the matrix $\\mathbf{A}$\n",
    "- Let $\\mathbf{b_i \\in \\Bbb{R}^k}$ denote the row vector representing the $i^{th}$ row of the matrix $\\mathbf{B}$\n",
    "\n",
    "$$\\begin{split}\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To form the matrix product $\\mathbf{C} \\in \\Bbb{R}^{n \\times m}$ we simply compute each element $c_{ij}$ as the dot product between the $i^{th}$ row of $\\mathbf{A}$ and the $j^{th}$ column of $\\mathbf{B}$, for example $\\mathbf{a}^\\top_i \\mathbf{b}_j$\n",
    "\n",
    "$$\\begin{split}\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The matrix-matrix multiplication $\\mathbf{AB}$ as performing $m$ matrix-vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]], dtype=torch.float64),\n",
       " tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=float).reshape(2,3)\n",
    "B = torch.ones(3,4, dtype=float)\n",
    "\n",
    "torch.mm(A,B), A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The term matrix–matrix multiplication is often simplified to matrix multiplication, and should not be confused with the Hadamard product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "- Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big it is\n",
    "- A norm is a function $\\| \\cdot \\|$ that maps a vector to a scalar and satisfies the following three properties:\n",
    "1. Given any vecotr $\\mathbf{x}$, if we scale (all elements of) the vector by a scalar $ \\alpha \\in \\Bbb{R}$, it norms scales accordingly:\n",
    "$$\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|.$$\n",
    "\n",
    "2. For any $\\mathbf{x}$ and $\\mathbf{y}$: norms satisfy the triangle inequality:\n",
    "$$\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|.$$\n",
    "\n",
    "3. The norm of a vector is nonegativa an only vanishes if the vector is zero:\n",
    "$$\\|\\mathbf{x}\\| > 0 \\textrm{ for all } \\mathbf{x} \\neq 0.$$\n",
    "\n",
    "- Many functions are valid norms and different norms encode different notions of size:\n",
    "- For vectors:  \n",
    "\n",
    "1. Manhattan distance ($\\ell_1$):\n",
    "$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Euclidean norm ($\\ell_2$):\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This norms are special cases of the more general $\\ell_p$ norms:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For matrices:\n",
    "\n",
    "1. Frobenius norm:\n",
    "\n",
    "$$\\|\\mathbf{X}\\|_\\textrm{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Spectral Norm:\n",
    "How much longer the matrix–vector product $\\mathbf{XV}$ could be relative to $\\mathbf{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These concepts are useful because we are often trying to solve optimization problems:\n",
    "\n",
    "- maximize the probability assigned to observed data\n",
    "- maximize the revenue associated with a recommender model\n",
    "- minimize the distance between predictions and the ground truth observations\n",
    "- minimize the distance between representations of photos of the same person while maximizing the distance between representations of photos of different people\n",
    "\n",
    "These distances, which constitute the objectives of deep learning algorithms, are often expressed as norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(2,3)\n",
    "A == (A.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(12).reshape(3,4)\n",
    "B = torch.arange(12).reshape(3,4)\n",
    "\n",
    "A.T + B.T == (A + B).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "$A+A^\\top = B\\\\\n",
    " B^\\top = (A+A^\\top)^\\top = (A^\\top+{A^{\\top}}^\\top) = A^\\top+A = A+A^\\top = B $\n",
    "\n",
    "$B = B^\\top$ implies $A+A^\\top = (A+A^\\top)^\\top $ therefore the matrix is always simmetrycal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]]),\n",
       " tensor([[ 0,  4,  8, 12],\n",
       "         [ 1,  5,  9, 13],\n",
       "         [ 2,  6, 10, 14],\n",
       "         [ 3,  7, 11, 15]]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(16).reshape(4,4)\n",
    "A, A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  5, 10, 15],\n",
       "         [ 5, 10, 15, 20],\n",
       "         [10, 15, 20, 25],\n",
       "         [15, 20, 25, 30]]),\n",
       " tensor([[ 0,  5, 10, 15],\n",
       "         [ 5, 10, 15, 20],\n",
       "         [10, 15, 20, 25],\n",
       "         [15, 20, 25, 30]]),\n",
       " tensor([[ 0,  5, 10, 15],\n",
       "         [ 5, 10, 15, 20],\n",
       "         [10, 15, 20, 25],\n",
       "         [15, 20, 25, 30]]),\n",
       " tensor([[ 0,  5, 10, 15],\n",
       "         [ 5, 10, 15, 20],\n",
       "         [10, 15, 20, 25],\n",
       "         [15, 20, 25, 30]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A + A.T, A.T + A, (A + A.T).T, (A.T + A).T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The len(X) will return the first value in the shape definition (first axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 4)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(3,4,2)\n",
    "Y = torch.arange(24).reshape(2,3,4)\n",
    "Z = torch.arange(24).reshape(4,3,2)\n",
    "len(X), len(Y), len(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes it always corresponds to the first axis that defines the number of elements corresponding to the most outer brackets []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0455, 0.0526, 0.0556],\n",
       "         [0.6667, 0.2273, 0.1579, 0.1296],\n",
       "         [1.3333, 0.4091, 0.2632, 0.2037],\n",
       "         [2.0000, 0.5909, 0.3684, 0.2778]]),\n",
       " 0.6666666666666666)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(16).reshape(4,4)\n",
    "\n",
    "A / A.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator is a vector with the sum of each row of the 4x4 Matrix, the numerator is a 4x4 matrix. The division is an elementwise division between each element of the vector with each row of the matrix in the same index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0455, 0.0526, 0.0556]),\n",
       " tensor([0.6667, 0.2273, 0.1579, 0.1296]),\n",
       " tensor([1.3333, 0.4091, 0.2632, 0.2037]),\n",
       " tensor([2.0000, 0.5909, 0.3684, 0.2778]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0] /A.sum(axis=1), A[1] /A.sum(axis=1), A[2] /A.sum(axis=1), A[3] /A.sum(axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(40.), tensor(31.6228))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avenue_dist = 30.0\n",
    "street_dist = 10.0\n",
    "\n",
    "distance_2pts = torch.tensor([avenue_dist, street_dist]) \n",
    "\n",
    "torch.abs(distance_2pts).sum(), torch.norm(distance_2pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2,3,4)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[12, 14, 16, 18],\n",
       "          [20, 22, 24, 26],\n",
       "          [28, 30, 32, 34]]),\n",
       "  torch.Size([3, 4])),\n",
       " (tensor([[12, 15, 18, 21],\n",
       "          [48, 51, 54, 57]]),\n",
       "  torch.Size([2, 4])),\n",
       " (tensor([[ 6, 22, 38],\n",
       "          [54, 70, 86]]),\n",
       "  torch.Size([2, 3])))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.sum(axis=0),X.sum(axis=0).shape), (X.sum(axis=1),X.sum(axis=1).shape), (X.sum(axis=2),X.sum(axis=2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(65.7571, dtype=torch.float64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24, dtype=float).reshape(2,3,4)\n",
    "\n",
    "torch.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.75712889109438"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "sqrt((X**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(65.7571, dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape(6,4)\n",
    "torch.linalg.norm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the square root of the sum of the squares of each element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A \\in \\Bbb{R}^{2^{10} \\times 2^{16}}, B \\in \\Bbb{R}^{2^{16} \\times 2^{5}}, C \\in \\Bbb{R}^{2^{5} \\times 2^{14}}$\n",
    "\n",
    "We are gonna to consider $X$ as an intermediate matrix to compute the Matrix product $ABC = Y$\n",
    "\n",
    "- $(AB)C$\n",
    "    1. $(AB) = X \\in \\Bbb{R}^{2^{10} \\times 2^{5}}$\n",
    "    2. $ XC = Y \\in \\Bbb{R}^{2^{10} \\times 2^{14}}$\n",
    "\n",
    "- $A(BC)$\n",
    "    1. $(BC) = X \\in \\Bbb{R}^{2^{16} \\times 2^{14}}$\n",
    "    2. $ AX = Y \\in \\Bbb{R}^{2^{10} \\times 2^{14}}$\n",
    "\n",
    "Considering that the computation gonna follow a sequential order computing the result of each matrix multiplication, the first approach is probably more generate less memory footprint than the second approach. That's because the intermediate matrix computed is much smaller than the intermediate matrix in the first approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "a = torch.randn([2**10,2**16])\n",
    "b = torch.randn([2**16,2**5])\n",
    "c = torch.randn([2**5,2**16])\n",
    "d = b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_0 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.063 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_1 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_2 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_3 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "\n",
      "iteration_4 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_5 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_6 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "\n",
      "iteration_7 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_8 :\n",
      "\n",
      "time to generate AxB: 0.044 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_9 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_10 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_11 :\n",
      "\n",
      "time to generate AxB: 0.038 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_12 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_13 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_14 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_15 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_16 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "\n",
      "iteration_17 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "\n",
      "iteration_18 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_19 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_20 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_21 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_22 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_23 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_24 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "\n",
      "iteration_25 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_26 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_27 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_28 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.035 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_29 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_30 :\n",
      "\n",
      "time to generate AxB: 0.038 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_31 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.035 seconds\n",
      "\n",
      "iteration_32 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "\n",
      "iteration_33 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "\n",
      "iteration_34 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "\n",
      "iteration_35 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "\n",
      "iteration_36 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_37 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_38 :\n",
      "\n",
      "time to generate AxB: 0.036 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_39 :\n",
      "\n",
      "time to generate AxB: 0.033 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_40 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_41 :\n",
      "\n",
      "time to generate AxB: 0.035 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_42 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_43 :\n",
      "\n",
      "time to generate AxB: 0.044 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_44 :\n",
      "\n",
      "time to generate AxB: 0.038 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_45 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_46 :\n",
      "\n",
      "time to generate AxB: 0.044 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_47 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "\n",
      "iteration_48 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_49 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_50 :\n",
      "\n",
      "time to generate AxB: 0.038 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_51 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_52 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_53 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_54 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_55 :\n",
      "\n",
      "time to generate AxB: 0.034 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_56 :\n",
      "\n",
      "time to generate AxB: 0.044 seconds\n",
      "time to generate AxC.T:0.034 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_57 :\n",
      "\n",
      "time to generate AxB: 0.036 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_58 :\n",
      "\n",
      "time to generate AxB: 0.035 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_59 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_60 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_61 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_62 :\n",
      "\n",
      "time to generate AxB: 0.037 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_63 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_64 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_65 :\n",
      "\n",
      "time to generate AxB: 0.038 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_66 :\n",
      "\n",
      "time to generate AxB: 0.037 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_67 :\n",
      "\n",
      "time to generate AxB: 0.038 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_68 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_69 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_70 :\n",
      "\n",
      "time to generate AxB: 0.045 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_71 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "\n",
      "iteration_72 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_73 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_74 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_75 :\n",
      "\n",
      "time to generate AxB: 0.037 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "\n",
      "iteration_76 :\n",
      "\n",
      "time to generate AxB: 0.044 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "\n",
      "iteration_77 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "\n",
      "iteration_78 :\n",
      "\n",
      "time to generate AxB: 0.044 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "time to generate AxC.T:0.034 seconds\n",
      "\n",
      "iteration_79 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_80 :\n",
      "\n",
      "time to generate AxB: 0.044 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_81 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_82 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.034 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_83 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_84 :\n",
      "\n",
      "time to generate AxB: 0.035 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_85 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_86 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.038 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "\n",
      "iteration_87 :\n",
      "\n",
      "time to generate AxB: 0.036 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_88 :\n",
      "\n",
      "time to generate AxB: 0.038 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "time to generate AxC.T:0.037 seconds\n",
      "\n",
      "iteration_89 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.036 seconds\n",
      "\n",
      "iteration_90 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "\n",
      "iteration_91 :\n",
      "\n",
      "time to generate AxB: 0.038 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_92 :\n",
      "\n",
      "time to generate AxB: 0.04 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "\n",
      "iteration_93 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_94 :\n",
      "\n",
      "time to generate AxB: 0.044 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "time to generate AxC.T:0.04 seconds\n",
      "\n",
      "iteration_95 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.039 seconds\n",
      "\n",
      "iteration_96 :\n",
      "\n",
      "time to generate AxB: 0.041 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "time to generate AxC.T:0.042 seconds\n",
      "\n",
      "iteration_97 :\n",
      "\n",
      "time to generate AxB: 0.042 seconds\n",
      "time to generate AxC.T:0.035 seconds\n",
      "time to generate AxC.T:0.044 seconds\n",
      "\n",
      "iteration_98 :\n",
      "\n",
      "time to generate AxB: 0.043 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "time to generate AxC.T:0.043 seconds\n",
      "\n",
      "iteration_99 :\n",
      "\n",
      "time to generate AxB: 0.039 seconds\n",
      "time to generate AxC.T:0.041 seconds\n",
      "time to generate AxC.T:0.045 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times =  {}\n",
    "for i in range(100):\n",
    "    key = f\"iteration_{i} :\"\n",
    "    times[key] = {}\n",
    "    print(f\"{key}\\n\")\n",
    "    start = time.time()\n",
    "    a@b\n",
    "    end = time.time()\n",
    "    times[key][\"time_A@B\"] = end-start\n",
    "    print (f\"time to generate AxB: {round(end-start,3)} seconds\") \n",
    "    start = time.time()\n",
    "    a@c.T\n",
    "    end = time.time()\n",
    "    times[key][\"time_A@CT\"] = end-start\n",
    "    print (f\"time to generate AxC.T:{round(end-start,3)} seconds\")\n",
    "    start = time.time()\n",
    "    a@d.T\n",
    "    end = time.time()\n",
    "    times[key][\"time_A@(BT)T\"] = end-start\n",
    "    print (f\"time to generate AxC.T:{round(end-start,3)} seconds\\n\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_A@B</th>\n",
       "      <th>time_A@CT</th>\n",
       "      <th>time_A@(BT)T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iteration_0 :</th>\n",
       "      <td>0.041001</td>\n",
       "      <td>0.062999</td>\n",
       "      <td>0.043999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_1 :</th>\n",
       "      <td>0.042002</td>\n",
       "      <td>0.038998</td>\n",
       "      <td>0.041995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_2 :</th>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.038998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_3 :</th>\n",
       "      <td>0.041003</td>\n",
       "      <td>0.040997</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_4 :</th>\n",
       "      <td>0.039003</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.042997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_95 :</th>\n",
       "      <td>0.041996</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>0.038995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_96 :</th>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.044003</td>\n",
       "      <td>0.041997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_97 :</th>\n",
       "      <td>0.042001</td>\n",
       "      <td>0.035001</td>\n",
       "      <td>0.044001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_98 :</th>\n",
       "      <td>0.042998</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.042998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_99 :</th>\n",
       "      <td>0.039001</td>\n",
       "      <td>0.040998</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                time_A@B  time_A@CT  time_A@(BT)T\n",
       "iteration_0 :   0.041001   0.062999      0.043999\n",
       "iteration_1 :   0.042002   0.038998      0.041995\n",
       "iteration_2 :   0.040002   0.044000      0.038998\n",
       "iteration_3 :   0.041003   0.040997      0.045000\n",
       "iteration_4 :   0.039003   0.041000      0.042997\n",
       "...                  ...        ...           ...\n",
       "iteration_95 :  0.041996   0.041006      0.038995\n",
       "iteration_96 :  0.041000   0.044003      0.041997\n",
       "iteration_97 :  0.042001   0.035001      0.044001\n",
       "iteration_98 :  0.042998   0.043000      0.042998\n",
       "iteration_99 :  0.039001   0.040998      0.045000\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(times).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_A@B</th>\n",
       "      <th>time_A@CT</th>\n",
       "      <th>time_A@(BT)T</th>\n",
       "      <th>difference_A@B_A@CT</th>\n",
       "      <th>difference_A@B_A@(BT)T</th>\n",
       "      <th>difference_A@CT_A@(BT)T</th>\n",
       "      <th>A@B_faster_A@CT</th>\n",
       "      <th>A@B_faster_A@(BT)T</th>\n",
       "      <th>A@CT_faster_A@(BT)T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iteration_0 :</th>\n",
       "      <td>0.041001</td>\n",
       "      <td>0.062999</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>-0.021998</td>\n",
       "      <td>-2.998114e-03</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_1 :</th>\n",
       "      <td>0.042002</td>\n",
       "      <td>0.038998</td>\n",
       "      <td>0.041995</td>\n",
       "      <td>0.003005</td>\n",
       "      <td>7.629395e-06</td>\n",
       "      <td>-0.002997</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_2 :</th>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.038998</td>\n",
       "      <td>-0.003998</td>\n",
       "      <td>1.004457e-03</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_3 :</th>\n",
       "      <td>0.041003</td>\n",
       "      <td>0.040997</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-3.996849e-03</td>\n",
       "      <td>-0.004003</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_4 :</th>\n",
       "      <td>0.039003</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.042997</td>\n",
       "      <td>-0.001997</td>\n",
       "      <td>-3.994465e-03</td>\n",
       "      <td>-0.001997</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_95 :</th>\n",
       "      <td>0.041996</td>\n",
       "      <td>0.041006</td>\n",
       "      <td>0.038995</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>3.001928e-03</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_96 :</th>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.044003</td>\n",
       "      <td>0.041997</td>\n",
       "      <td>-0.003003</td>\n",
       "      <td>-9.975433e-04</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_97 :</th>\n",
       "      <td>0.042001</td>\n",
       "      <td>0.035001</td>\n",
       "      <td>0.044001</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>-1.999855e-03</td>\n",
       "      <td>-0.009001</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_98 :</th>\n",
       "      <td>0.042998</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.042998</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-2.384186e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iteration_99 :</th>\n",
       "      <td>0.039001</td>\n",
       "      <td>0.040998</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>-0.001997</td>\n",
       "      <td>-5.999327e-03</td>\n",
       "      <td>-0.004002</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                time_A@B  time_A@CT  time_A@(BT)T  difference_A@B_A@CT  \\\n",
       "iteration_0 :   0.041001   0.062999      0.043999            -0.021998   \n",
       "iteration_1 :   0.042002   0.038998      0.041995             0.003005   \n",
       "iteration_2 :   0.040002   0.044000      0.038998            -0.003998   \n",
       "iteration_3 :   0.041003   0.040997      0.045000             0.000006   \n",
       "iteration_4 :   0.039003   0.041000      0.042997            -0.001997   \n",
       "...                  ...        ...           ...                  ...   \n",
       "iteration_95 :  0.041996   0.041006      0.038995             0.000991   \n",
       "iteration_96 :  0.041000   0.044003      0.041997            -0.003003   \n",
       "iteration_97 :  0.042001   0.035001      0.044001             0.007001   \n",
       "iteration_98 :  0.042998   0.043000      0.042998            -0.000002   \n",
       "iteration_99 :  0.039001   0.040998      0.045000            -0.001997   \n",
       "\n",
       "                difference_A@B_A@(BT)T  difference_A@CT_A@(BT)T  \\\n",
       "iteration_0 :            -2.998114e-03                 0.019000   \n",
       "iteration_1 :             7.629395e-06                -0.002997   \n",
       "iteration_2 :             1.004457e-03                 0.005002   \n",
       "iteration_3 :            -3.996849e-03                -0.004003   \n",
       "iteration_4 :            -3.994465e-03                -0.001997   \n",
       "...                                ...                      ...   \n",
       "iteration_95 :            3.001928e-03                 0.002011   \n",
       "iteration_96 :           -9.975433e-04                 0.002005   \n",
       "iteration_97 :           -1.999855e-03                -0.009001   \n",
       "iteration_98 :           -2.384186e-07                 0.000001   \n",
       "iteration_99 :           -5.999327e-03                -0.004002   \n",
       "\n",
       "                A@B_faster_A@CT  A@B_faster_A@(BT)T  A@CT_faster_A@(BT)T  \n",
       "iteration_0 :              True                True                False  \n",
       "iteration_1 :             False               False                 True  \n",
       "iteration_2 :              True               False                False  \n",
       "iteration_3 :             False                True                 True  \n",
       "iteration_4 :              True                True                 True  \n",
       "...                         ...                 ...                  ...  \n",
       "iteration_95 :            False               False                False  \n",
       "iteration_96 :             True                True                False  \n",
       "iteration_97 :            False                True                 True  \n",
       "iteration_98 :             True                True                False  \n",
       "iteration_99 :             True                True                 True  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"difference_A@B_A@CT\"] = df.iloc[:,0] - df.iloc[:,1]\n",
    "df[\"difference_A@B_A@(BT)T\"] = df.iloc[:,0] - df.iloc[:,2]\n",
    "df[\"difference_A@CT_A@(BT)T\"] = df.iloc[:,1] - df.iloc[:,2]\n",
    "df[\"A@B_faster_A@CT\"] = df[\"difference_A@B_A@CT\"] < 0\n",
    "df[\"A@B_faster_A@(BT)T\"] = df[\"difference_A@B_A@(BT)T\"] < 0\n",
    "df[\"A@CT_faster_A@(BT)T\"] = df[\"difference_A@CT_A@(BT)T\"] < 0\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(A@B_faster_A@CT\n",
       " True     52\n",
       " False    48\n",
       " Name: count, dtype: int64,\n",
       " A@B_faster_A@(BT)T\n",
       " True     53\n",
       " False    47\n",
       " Name: count, dtype: int64,\n",
       " A@CT_faster_A@(BT)T\n",
       " False    53\n",
       " True     47\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"A@B_faster_A@CT\"].value_counts(), df[\"A@B_faster_A@(BT)T\"].value_counts(), df[\"A@CT_faster_A@(BT)T\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems like is a little faster to compute $AB$ rhater than $AC^{\\top}$ \n",
    "- There was runnings in wich  $A(B^\\top)^\\top$ was faster than $AB$ but most of the times $AB$ was faster.\n",
    "- There wasn't a significant difference between $A(B^\\top)^\\top$ and $AC^{\\top}$ sometimes the first what faster and sometimes the second was faster.\n",
    "- The elements are initialized randomly so probably have some influence in the time it tooks to compute the multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones(100, 200, dtype=torch.int32)\n",
    "B = A*2\n",
    "C = A*3\n",
    "\n",
    "X = torch.stack([A,B,C])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensio of the tensor is (3, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100, 200])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X[1] == B).prod()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
